{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12589284,"sourceType":"datasetVersion","datasetId":7891595}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -U FlagEmbedding[finetune]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T07:48:07.721077Z","iopub.execute_input":"2025-07-27T07:48:07.721338Z","iopub.status.idle":"2025-07-27T07:50:26.544550Z","shell.execute_reply.started":"2025-07-27T07:48:07.721317Z","shell.execute_reply":"2025-07-27T07:50:26.543884Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-tune","metadata":{}},{"cell_type":"markdown","source":"Below are the arguments for fine-tuning:\n\nThe following arguments are for model:\n- `model_name_or_path`: The model checkpoint for initialization.\n- `config_name`: Pretrained config name or path if not the same as model_name.\n- `tokenizer_name`: Pretrained tokenizer name or path if not the same as model_name.\n- `cache_dir`: Where do you want to store the pre-trained models downloaded from s3.\n- `trust_remote_code`: Trust remote code\n- `token`: The token to use when accessing the model.\n\nThe following arguments are for data:\n- `train_data`: One or more paths to training data. `query: str`, `pos: List[str]`, `neg: List[str]` are required in the training data. Argument type: multiple.\n- `cache_path`: Where do you want to store the cached data.\n- `train_group_size`: (No metadata provided)\n- `query_max_len`: The maximum total input sequence length after tokenization for passage. Sequences longer than this will be truncated.\n- `passage_max_len`: The maximum total input sequence length after tokenization for passage. Sequences longer than this will be truncated.\n- `pad_to_multiple_of`: If set will pad the sequence to be a multiple of the provided value.\n- `max_example_num_per_dataset`: The max number of examples for each dataset.\n- `query_instruction_for_retrieval`: Instruction for query.\n- `query_instruction_format`: Format for query instruction.\n- `knowledge_distillation`: Use knowledge distillation when `pos_scores: List[float]` and `neg_scores: List[float]` are in features of training data.\n- `passage_instruction_for_retrieval`: Instruction for passage.\n- `passage_instruction_format`: Format for passage instruction.\n- `shuffle_ratio`: The ratio of shuffling the text.\n- `same_dataset_within_batch`: All samples in the same batch comes from the same dataset.\n- `small_threshold`: The threshold of small dataset. All small dataset in the same directory will be merged into one dataset.\n- `drop_threshold`: The threshold for dropping merged small dataset. If the number of examples in the merged small dataset is less than this threshold, it will be dropped.\n\nAnd the following extra arguments:\n- `negatives_cross_device`: Share negatives across devices.\n- `temperature`: Temperature used for similarity score.\n- `fix_position_embedding`: Freeze the parameters of position embeddings.\n- `sentence_pooling_method`: The pooling method. Available options: cls, mean, last_token. Default: cls.\n- `normalize_embeddings`: Whether to normalize the embeddings.\n- `sub_batch_size`: Sub batch size for training.\n- `kd_loss_type`: The loss type for knowledge distillation. Available options: kl_div, m3_kd_loss. Default: kl_div.","metadata":{}},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"5075b85e4e708e828d33dba7fcc413dcdecbe4c8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T07:50:26.545842Z","iopub.execute_input":"2025-07-27T07:50:26.546072Z","iopub.status.idle":"2025-07-27T07:50:35.072679Z","shell.execute_reply.started":"2025-07-27T07:50:26.546048Z","shell.execute_reply":"2025-07-27T07:50:35.072096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) Gỡ sạch flash-attn\n!pip uninstall -y flash-attn\n\n# 2) Thiết biến môi trường để tắt import Flash-Attention\n%env HF_NO_FLASH_ATTN=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T07:50:35.073248Z","iopub.execute_input":"2025-07-27T07:50:35.073555Z","iopub.status.idle":"2025-07-27T07:51:06.555576Z","shell.execute_reply.started":"2025-07-27T07:50:35.073528Z","shell.execute_reply":"2025-07-27T07:51:06.554630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\ntorchrun --nproc_per_node 2 \\\n  -m FlagEmbedding.finetune.reranker.encoder_only.base \\\n  --model_name_or_path BAAI/bge-reranker-v2-m3 \\\n  --cache_dir /kaggle/working/cache/model \\\n  --train_data /kaggle/input/ft-data/training_512.json \\\n  --cache_path /kaggle/working/cache/data \\\n  --train_group_size 12 \\\n  --query_max_len 512 \\\n  --passage_max_len 1024 \\\n  --pad_to_multiple_of 8 \\\n  --query_instruction_for_rerank 'Với vai trò là một chuyên gia pháp luật, hãy tìm kiếm các điều khoản, quy định pháp luật có liên quan trực tiếp đến vấn đề: ' \\\n  --query_instruction_format '{}{}' \\\n  --knowledge_distillation False \\\n  --output_dir test_encoder_only_base_bge-reranker-v2-m3 \\\n  --overwrite_output_dir \\\n  --learning_rate 5e-6 \\\n  --bf16 \\\n  --num_train_epochs 20 \\\n  --per_device_train_batch_size 8 \\\n  --gradient_accumulation_steps 16 \\\n  --dataloader_drop_last True \\\n  --warmup_ratio 0.1 \\\n  --deepspeed /kaggle/input/ft-data/ds_stage0.json \\\n  --logging_steps 1 \\\n  --save_steps 500 \\\n  --gradient_checkpointing \\\n  --save_total_limit 3 \\\n  --report_to wandb \\\n  --run_name \"finetune-bge-$(date +%Y%m%d_%H%M)\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:17:14.217002Z","iopub.execute_input":"2025-07-27T08:17:14.217725Z","iopub.status.idle":"2025-07-27T08:26:38.909901Z","shell.execute_reply.started":"2025-07-27T08:17:14.217684Z","shell.execute_reply":"2025-07-27T08:26:38.908883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport shutil\n\n# Chuyển vào thư mục chứa các folder checkpoint-*\nos.chdir('/kaggle/working/test_encoder_only_base_bge-reranker-v2-m3')\n\n# Lấy danh sách các folder checkpoint-*\ncheckpoint_dirs = [d for d in glob.glob('checkpoint-*') if os.path.isdir(d)]\nif not checkpoint_dirs:\n    print(\"Không tìm thấy folder nào thỏa pattern 'checkpoint-*'\")\nelse:\n    print(f\"Đã tìm thấy {len(checkpoint_dirs)} checkpoint(s):\")\n    for d in checkpoint_dirs:\n        print(\"  -\", d)\n\n# 1) Zip từng folder checkpoint-xxx thành các file riêng\nfor chk in checkpoint_dirs:\n    zip_name = shutil.make_archive(chk, 'zip', root_dir=chk)\n    print(f\"Đã tạo archive: {zip_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T08:33:29.689436Z","iopub.execute_input":"2025-07-27T08:33:29.690174Z","iopub.status.idle":"2025-07-27T08:33:29.697002Z","shell.execute_reply.started":"2025-07-27T08:33:29.690150Z","shell.execute_reply":"2025-07-27T08:33:29.696443Z"}},"outputs":[],"execution_count":null}]}